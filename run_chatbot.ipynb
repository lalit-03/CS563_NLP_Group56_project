{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4qlhA50xpg7M"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install --upgrade -q -U bitsandbytes\n",
        "!pip3 install --upgrade -q -U peft\n",
        "!pip3 install --upgrade -q -U trl\n",
        "!pip3 install --upgrade -q -U accelerate\n",
        "!pip3 install --upgrade -q -U datasets\n",
        "!pip install evaluate rouge_score bert_score sacrebleu nltk sentencepiece accelerate # Need accelerate & sentencepiece for some BERTScore models\n",
        "!pip3 install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSQij46rpj6B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = 'huggingface_token'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huji0Olmpmdg",
        "outputId": "257f6267-6c09-40b5-d214-a3c35cc79f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading base model 'google/gemma-3-1b-it' for summarizer...\n",
            "Loading PEFT adapter 'lalit-03/gemma3-1b-summarization-finetuned' for summarizer...\n",
            "Summarizer model loaded.\n",
            "Summarizer tokenizer loaded.\n",
            "Loading generator model 'google/gemma-3-1b-it'...\n",
            "Generator model loaded.\n",
            "Generator tokenizer loaded.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    Gemma3ForCausalLM, # Use the correct Gemma 3 class\n",
        "    GenerationConfig\n",
        ")\n",
        "from peft import PeftModel\n",
        "import readline # Optional: for better terminal input experience\n",
        "\n",
        "# --- Configuration ---\n",
        "SUMMARIZER_ADAPTER_ID = \"lalit-03/gemma3-1b-summarization-finetuned\"\n",
        "BASE_MODEL_ID = \"google/gemma-3-1b-it\" # Base for both summarizer and generator\n",
        "GENERATOR_MODEL_ID = \"google/gemma-3-1b-it\" # Can be the same or different\n",
        "\n",
        "# Quantization Config (apply to both models for memory efficiency)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    # bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 if supported\n",
        "    bnb_4bit_compute_dtype=torch.float16 # Fallback to float16 if bf16 not supported\n",
        ")\n",
        "\n",
        "# --- Device Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Load Summarization Model (Base + Adapter) ---\n",
        "print(f\"Loading base model '{BASE_MODEL_ID}' for summarizer...\")\n",
        "base_model_summarizer = Gemma3ForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\", # Let HF handle device placement\n",
        "    attn_implementation=\"eager\", # Or \"sdpa\" if available/preferred,\n",
        ")\n",
        "\n",
        "print(f\"Loading PEFT adapter '{SUMMARIZER_ADAPTER_ID}' for summarizer...\")\n",
        "summarizer_model = PeftModel.from_pretrained(base_model_summarizer, SUMMARIZER_ADAPTER_ID)\n",
        "summarizer_model.eval() # Set to evaluation mode\n",
        "print(\"Summarizer model loaded.\")\n",
        "\n",
        "# --- Load Summarizer Tokenizer ---\n",
        "# Use the tokenizer associated with the adapter's base model\n",
        "summarizer_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
        "if summarizer_tokenizer.pad_token is None:\n",
        "    summarizer_tokenizer.pad_token = summarizer_tokenizer.eos_token\n",
        "    # Ensure the underlying model config is updated ONLY IF NECESSARY\n",
        "    # (often handled by from_pretrained with PEFT, but good to be aware)\n",
        "    # summarizer_model.config.pad_token_id = summarizer_tokenizer.pad_token_id\n",
        "summarizer_tokenizer.padding_side = \"left\" # Use left padding for generation\n",
        "print(\"Summarizer tokenizer loaded.\")\n",
        "\n",
        "\n",
        "# --- Load Response Generation Model ---\n",
        "print(f\"Loading generator model '{GENERATOR_MODEL_ID}'...\")\n",
        "generator_model = Gemma3ForCausalLM.from_pretrained(\n",
        "    GENERATOR_MODEL_ID,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\", # Let HF handle device placement\n",
        "    attn_implementation=\"eager\" # Or \"sdpa\"\n",
        ")\n",
        "generator_model.eval() # Set to evaluation mode\n",
        "print(\"Generator model loaded.\")\n",
        "\n",
        "# --- Load Generator Tokenizer ---\n",
        "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL_ID)\n",
        "if generator_tokenizer.pad_token is None:\n",
        "    generator_tokenizer.pad_token = generator_tokenizer.eos_token\n",
        "    # generator_model.config.pad_token_id = generator_tokenizer.pad_token_id\n",
        "generator_tokenizer.padding_side = \"left\" # Use left padding for generation\n",
        "print(\"Generator tokenizer loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xE5lDOI9qsm4"
      },
      "outputs": [],
      "source": [
        "# --- Helper Functions ---\n",
        "\n",
        "def format_history_for_summarizer(history):\n",
        "    \"\"\"\n",
        "    Formats the conversation history into a string similar to the\n",
        "    training data format for the summarizer. Adjust this function\n",
        "    if your training format was different.\n",
        "    \"\"\"\n",
        "    formatted_conversation = \"\"\n",
        "    for turn in history:\n",
        "        speaker = \"User\" if turn[\"role\"] == \"user\" else \"Bot\"\n",
        "        formatted_conversation += f\">>> {speaker}: {turn['content']}\\n\"\n",
        "\n",
        "    # Construct the prompt exactly as used in training\n",
        "    prompt = f\"\"\"Instruction: Please summarize the following empathetic dialogue conversation.\n",
        "\n",
        "### Conversation:\n",
        "{formatted_conversation.strip()}\n",
        "\n",
        "### Summary:\n",
        "\"\"\" # The model should generate text after this marker\n",
        "    return prompt\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_summary(conversation_history):\n",
        "    \"\"\"Generates a summary of the conversation history.\"\"\"\n",
        "\n",
        "    prompt = format_history_for_summarizer(conversation_history)\n",
        "    inputs = summarizer_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(summarizer_model.device) # Truncate input prompt\n",
        "\n",
        "    generation_config_summary = GenerationConfig(\n",
        "        max_new_tokens=100, # Adjust max length for summary\n",
        "        pad_token_id=summarizer_tokenizer.pad_token_id,\n",
        "        eos_token_id=summarizer_tokenizer.eos_token_id,\n",
        "        do_sample=False, # Use greedy decoding or beam search for factual summary\n",
        "        num_beams=3,     # Example using beam search\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    outputs = summarizer_model.generate(**inputs, generation_config=generation_config_summary)\n",
        "\n",
        "    # Decode only the generated part (after the prompt)\n",
        "    input_length = inputs.input_ids.shape[1]\n",
        "    generated_ids = outputs[:, input_length:]\n",
        "    summary = summarizer_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Basic clean-up (remove potential leftover instruction/markers if any)\n",
        "    summary = summary.replace(\"### Summary:\", \"\").strip()\n",
        "\n",
        "    return summary\n",
        "\n",
        "from openai import OpenAI\n",
        "groq_client = OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=\"groq_api_key\",  # üîÅ Replace with your actual key\n",
        ")\n",
        "\n",
        "# GROQ setup\n",
        "# openai.api_key = \"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\"  # Replace with your actual API key\n",
        "# openai.api_base = \"https://api.groq.com/openai/v1\"\n",
        "\n",
        "def get_empathetic_response(summary, conversation_history):\n",
        "    \"\"\"Generates an empathetic response using a large model from GROQ API.\"\"\"\n",
        "    if not conversation_history:\n",
        "        return \"Hello! How can I help you today?\"\n",
        "\n",
        "    last_user_message = \"\"\n",
        "    for i in range(len(conversation_history) - 1, -1, -1):\n",
        "        if conversation_history[i][\"role\"] == \"user\":\n",
        "            last_user_message = conversation_history[i][\"content\"]\n",
        "            break\n",
        "\n",
        "    if not last_user_message:\n",
        "        last_user_message = conversation_history[-1]['content']\n",
        "\n",
        "    # Construct the same instruction-based prompt (modified version you approved)\n",
        "    prompt = f\"\"\"You are an empathetic chatbot. Your goal is to provide supportive and understanding responses, while matching the user's tone appropriately.\n",
        "If the user's last message is a casual or friendly greeting (like \"hi\", \"hello\", or \"how are you\"), respond in a warm and casual way.\n",
        "If the user's last message expresses emotional distress, respond with empathy, care, and understanding.\n",
        "Base your reply *only* on the following summary of the conversation so far, and the user's very last message.\n",
        "Do not refer to the summary directly in your response.\n",
        "\n",
        "Conversation Summary:\n",
        "{summary}\n",
        "\n",
        "User's Last Message:\n",
        "{last_user_message}\n",
        "\n",
        "Your Response:\n",
        "\"\"\"\n",
        "\n",
        "    # Call GROQ API\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=\"llama3-70b-8192\",  # Or llama3-70b if available on GROQ\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_tokens=200,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zZJy-k47yp1",
        "outputId": "240488f7-db01-4db6-f9a1-82fe5cba1392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Empathetic Chatbot ---\n",
            "Chatbot is ready. Type 'quit' to exit.\n",
            "You: Hi\n",
            "Skipping summary for the first turn.\n",
            "Generating response...\n",
            "Bot: Hey! It's great to chat with you! How's your day going so far?\n",
            "You: my girlfriend left me\n",
            "\n",
            "Generating summary...\n",
            "Summary generated: In a moment of vulnerability, the user expresses feelings of abandonment and hurt after their girlfriend has broken up with them. The bot responds wit...\n",
            "Generating response...\n",
            "Bot: I'm so sorry to hear that. It can be really tough to go through a breakup, especially when it feels like someone you care about is walking away. Would you like to talk about what's been going on and how you're feeling? I'm here to listen and offer support.\n",
            "You: her memories haunt me\n",
            "\n",
            "Generating summary...\n",
            "Summary generated: In a moment of vulnerability, the user expresses deep pain from a recent breakup, feeling that their ex-girlfriend has left them with painful memories...\n",
            "Generating response...\n",
            "Bot: I'm so sorry to hear that. It's like she's still with you, but in a way that's really painful. It's like your mind is replaying all the moments you shared, but now they're tainted with the sadness of what's been lost. That can be really overwhelming. Would you like to talk more about what's been going through your mind lately? I'm here to listen and support you through this tough time.\n",
            "You: but I am happy as she left me, she was toxic \n",
            "\n",
            "Generating summary...\n",
            "Summary generated: In a conversation marked by deep sadness, the user expresses pain over their girlfriend's departure, feeling that her memories are haunting them. The ...\n",
            "Generating response...\n",
            "Bot: It sounds like you're feeling a mix of emotions about the breakup. On one hand, you're relieved to be free from a toxic relationship, but on the other hand, the memories of her still linger and can be painful. That's completely understandable. It's like your heart is still catching up with your head, you know? You're acknowledging the unhealthy dynamics of the relationship, but it's natural to still feel some emotional residue. Would you like to talk more about what you're feeling right now?\n",
            "You: i really needed a closure\n",
            "\n",
            "Generating summary...\n",
            "Summary generated: The conversation revolves around the emotional turmoil of a user who recently ended a relationship with their girlfriend, who left them with painful m...\n",
            "Generating response...\n",
            "Bot: I can only imagine how much closure means to you right now. It's like finally getting a sense of resolution, even if the memories of your relationship still linger. It takes so much courage to acknowledge what you need, and I'm here to support you in that. Would you like to talk more about what closure looks like for you, and how you're hoping to achieve it?\n",
            "You: quit\n",
            "Bot: Goodbye! Take care.\n"
          ]
        }
      ],
      "source": [
        "# --- Main Chat Loop ---\n",
        "conversation_history = []\n",
        "\n",
        "print(\"\\n--- Empathetic Chatbot ---\")\n",
        "print(\"Chatbot is ready. Type 'quit' to exit.\")\n",
        "\n",
        "# Optional: Initial bot message\n",
        "# initial_bot_message = \"Hello there! I'm here to listen. How are you feeling today?\"\n",
        "# print(f\"Bot: {initial_bot_message}\")\n",
        "# conversation_history.append({\"role\": \"assistant\", \"content\": initial_bot_message})\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \")\n",
        "    except EOFError: # Handle Ctrl+D\n",
        "        print(\"\\nExiting chatbot.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Bot: Goodbye! Take care.\")\n",
        "        break\n",
        "\n",
        "    if not user_input.strip(): # Skip empty input\n",
        "        continue\n",
        "\n",
        "    # Add user message to history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # 1. Generate Summary (only if history exists)\n",
        "    current_summary = \"\"\n",
        "    if len(conversation_history) > 1:\n",
        "        print(\"\\nGenerating summary...\")\n",
        "        current_summary = get_summary(conversation_history)\n",
        "        print(f\"Summary generated: {current_summary[:150]}...\") # Print snippet\n",
        "    else:\n",
        "        print(\"Skipping summary for the first turn.\")\n",
        "\n",
        "\n",
        "    # 2. Generate Empathetic Response\n",
        "    print(\"Generating response...\")\n",
        "    bot_response = get_empathetic_response(current_summary, conversation_history)\n",
        "\n",
        "    # Add bot response to history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "    # Print bot response\n",
        "    print(f\"Bot: {bot_response}\")\n",
        "\n",
        "    # Optional: Limit history size to prevent excessive memory usage/context length issues\n",
        "    MAX_HISTORY_TURNS = 10 # Keep last 10 pairs (user + bot)\n",
        "    if len(conversation_history) > MAX_HISTORY_TURNS * 2:\n",
        "       # Keep the last MAX_HISTORY_TURNS*2 items (10 user, 10 bot)\n",
        "       conversation_history = conversation_history[-(MAX_HISTORY_TURNS * 2):]\n",
        "       print(\"(History trimmed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U1cWkfCU72Tk"
      },
      "outputs": [],
      "source": [
        "i"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
